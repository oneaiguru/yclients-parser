#!/usr/bin/env python3
"""
YClients Parser - –õ—ë–≥–∫–∞—è –≤–µ—Ä—Å–∏—è –±–µ–∑ Playwright
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç requests + BeautifulSoup –¥–ª—è –Ω–∞–¥—ë–∂–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ –±–µ–∑ –±—Ä–∞—É–∑–µ—Ä–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
"""
import os
import asyncio
import json
import re
from datetime import datetime
from typing import List, Dict, Optional, Any
import requests
from bs4 import BeautifulSoup
import asyncpg
from fastapi import FastAPI, HTTPException, Query
from fastapi.responses import HTMLResponse
import uvicorn
import logging

# –°–£–ü–ï–†–ü–û–ü–†–ê–í–ö–ê: –ò–º–ø–æ—Ä—Ç —Ä–µ–∞–ª—å–Ω–æ–≥–æ DatabaseManager –¥–ª—è Supabase –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
import sys
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)

try:
    from src.database.db_manager import DatabaseManager
    from src.parser.parser_router import ParserRouter
    SUPABASE_INTEGRATION_AVAILABLE = True
    print("‚úÖ SUPABASE INTEGRATION: –ó–∞–≥—Ä—É–∂–µ–Ω DatabaseManager")
    print("‚úÖ PARSER ROUTER: –ó–∞–≥—Ä—É–∂–µ–Ω ParserRouter")
except ImportError:
    SUPABASE_INTEGRATION_AVAILABLE = False
    print("‚ùå SUPABASE INTEGRATION: DatabaseManager –Ω–µ –Ω–∞–π–¥–µ–Ω")

# –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
API_HOST = os.environ.get("API_HOST", "0.0.0.0")
API_PORT = int(os.environ.get("API_PORT", "8000"))
API_KEY = os.environ.get("API_KEY", "default_key")
PARSE_URLS = os.environ.get("PARSE_URLS", "")
SUPABASE_URL = os.environ.get("SUPABASE_URL", "")
SUPABASE_KEY = os.environ.get("SUPABASE_KEY", "")
PARSE_INTERVAL = int(os.environ.get("PARSE_INTERVAL", "600"))

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
parsing_active = False
last_parse_time = None
parse_results = {"total_extracted": 0, "status": "–≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ"}

# –°–£–ü–ï–†–ü–û–ü–†–ê–í–ö–ê: –ì–ª–æ–±–∞–ª—å–Ω—ã–π DatabaseManager –¥–ª—è Supabase
db_manager = None

# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# FastAPI –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
app = FastAPI(
    title="–ü–∞—Ä—Å–µ—Ä YClients - –õ—ë–≥–∫–∞—è –≤–µ—Ä—Å–∏—è",
    description="–ü–∞—Ä—Å–µ—Ä –¥–∞–Ω–Ω—ã—Ö –±—Ä–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è YClients –±–µ–∑ –±—Ä–∞—É–∑–µ—Ä–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π",
    version="4.1.0"
)

class YClientsParser:
    """–õ—ë–≥–∫–∏–π –ø–∞—Ä—Å–µ—Ä YClients –Ω–∞ –æ—Å–Ω–æ–≤–µ requests + BeautifulSoup"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
    
    def is_javascript_heavy_page(self, soup: BeautifulSoup, url: str) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ JavaScript-—Ç—è–∂–µ–ª–æ–π (—Ç—Ä–µ–±—É–µ—Ç –±—Ä–∞—É–∑–µ—Ä–Ω–æ–≥–æ —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞)"""
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ JS –∫ –∫–æ–Ω—Ç–µ–Ω—Ç—É
        js_scripts = soup.find_all('script')
        js_size = sum(len(script.get_text()) for script in js_scripts if script.get_text())
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç (–∏—Å–∫–ª—é—á–∞—è —Å–∫—Ä–∏–ø—Ç—ã –∏ —Å—Ç–∏–ª–∏)
        for script in soup(["script", "style"]):
            script.decompose()
        text_content = soup.get_text()
        content_size = len(text_content.strip())
        
        logger.info(f"üìä –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {url}: JS={js_size} –±–∞–π—Ç, –∫–æ–Ω—Ç–µ–Ω—Ç={content_size} –±–∞–π—Ç")
        
        # –ï—Å–ª–∏ JS –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –±–æ–ª—å—à–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞, –≤–µ—Ä–æ—è—Ç–Ω–æ —ç—Ç–æ SPA
        if js_size > content_size * 2 and content_size < 1000:
            logger.info(f"üîç –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ SPA: JS({js_size}) >> –∫–æ–Ω—Ç–µ–Ω—Ç({content_size})")
            return True
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ YClients –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è SPA
        yclients_spa_indicators = [
            'yclients.com/company/',
            'record-type?o=',
            'personal/select-time',
            'personal/menu'
        ]
        
        if any(indicator in url for indicator in yclients_spa_indicators):
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –±—Ä–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –≤ HTML
            booking_indicators = soup.find_all(text=re.compile(r'\d{1,2}:\d{2}'))
            price_indicators = soup.find_all(text=re.compile(r'\d+\s*‚ÇΩ|\d+\s*—Ä—É–±'))
            
            if len(booking_indicators) == 0 and len(price_indicators) == 0:
                logger.info(f"üéØ YClients URL –±–µ–∑ –¥–∞–Ω–Ω—ã—Ö –±—Ä–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –≤ HTML - —Ç—Ä–µ–±—É–µ—Ç JavaScript")
                return True
        
        return False
    
    def parse_url(self, url: str) -> List[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–≥–æ URL —Å –ø–æ–º–æ—â—å—é requests"""
        try:
            logger.info(f"üéØ –ü–∞—Ä—Å–∏–Ω–≥ URL: {url}")
            
            # –ò–°–ü–†–ê–í–õ–ï–ù–û: –î–ª—è YClients URL –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä
            if 'yclients.com' in url:
                logger.info(f"üéØ YClients URL –æ–±–Ω–∞—Ä—É–∂–µ–Ω - –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä")
                from src.parser.lightweight_yclients_parser import LightweightYClientsParser
                yclients_parser = LightweightYClientsParser()
                booking_data = yclients_parser.parse_url(url)
                logger.info(f"‚úÖ YClients –ø–∞—Ä—Å–µ—Ä –∏–∑–≤–ª–µ–∫ {len(booking_data)} –∑–∞–ø–∏—Å–µ–π —Å {url}")
                return booking_data
            
            # –ü–æ–ª—É—á–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            # –ü–∞—Ä—Å–∏–º HTML
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ JavaScript-—Ç—è–∂–µ–ª–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ–π
            if self.is_javascript_heavy_page(soup, url):
                logger.info(f"üîß –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ JavaScript-—Ç—è–∂–µ–ª–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞: {url}")
                logger.info(f"üí° –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä")
                # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º —Å–æ–æ–±—â–µ–Ω–∏–µ–º
                return []
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –±—Ä–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
            booking_data = self.extract_booking_data_from_html(soup, url)
            
            logger.info(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(booking_data)} –∑–∞–ø–∏—Å–µ–π —Å {url}")
            return booking_data
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {url}: {e}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ - –ù–ï–¢ –î–ï–ú–û-–î–ê–ù–ù–´–•
            return []
    
    def extract_booking_data_from_html(self, soup: BeautifulSoup, url: str) -> List[Dict]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –±—Ä–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑ HTML"""
        booking_data = []
        
        try:
            # –ü–æ–∏—Å–∫ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ —Å–ª–æ—Ç–∞–º–∏
            time_elements = soup.find_all(text=re.compile(r'\d{1,2}:\d{2}'))
            
            # –ü–æ–∏—Å–∫ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Å —Ü–µ–Ω–∞–º–∏
            price_elements = soup.find_all(text=re.compile(r'\d+\s*‚ÇΩ|\d+\s*—Ä—É–±'))
            
            # –ü–æ–∏—Å–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–æ—Ä—Ç–∞—Ö/—É—Å–ª—É–≥–∞—Ö
            service_elements = soup.find_all(text=re.compile(r'–∫–æ—Ä—Ç|–∑–∞–ª|–ø–ª–æ—â–∞–¥–∫–∞', re.IGNORECASE))
            
            logger.info(f"üîç –ù–∞–π–¥–µ–Ω–æ: {len(time_elements)} –≤—Ä–µ–º—ë–Ω, {len(price_elements)} —Ü–µ–Ω, {len(service_elements)} —É—Å–ª—É–≥")
            
            # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –¥–∞–Ω–Ω—ã–µ, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏—Ö
            if time_elements or price_elements:
                # –°–æ–∑–¥–∞—ë–º –∑–∞–ø–∏—Å–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                for i in range(max(len(time_elements), len(price_elements), 3)):
                    
                    # –í—Ä–µ–º—è
                    time_text = None
                    if i < len(time_elements):
                        time_match = re.search(r'\d{1,2}:\d{2}', str(time_elements[i]))
                        if time_match:
                            time_text = time_match.group()
                    
                    if not time_text:
                        time_text = f"{10 + i}:00"
                    
                    # –¶–µ–Ω–∞
                    price_text = "–¶–µ–Ω–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞"
                    if i < len(price_elements):
                        price_match = re.search(r'\d+\s*‚ÇΩ|\d+\s*—Ä—É–±', str(price_elements[i]))
                        if price_match:
                            price_text = price_match.group()
                    
                    # –ü—Ä–æ–≤–∞–π–¥–µ—Ä
                    provider = "–ü–ª–æ—â–∞–¥–∫–∞ YClients"
                    if i < len(service_elements):
                        service_text = str(service_elements[i]).strip()
                        if service_text and len(service_text) < 50:
                            provider = service_text
                    
                    booking_slot = {
                        "url": url,
                        "date": datetime.now().strftime("%Y-%m-%d"),
                        "time": time_text,
                        "price": price_text,
                        "provider": provider,
                        "seat_number": str(i + 1),
                        "location_name": "YClients –ø–ª–æ—â–∞–¥–∫–∞",
                        "court_type": "GENERAL",
                        "time_category": "–î–ï–ù–¨" if int(time_text.split(":")[0]) < 17 else "–í–ï–ß–ï–†",
                        "duration": 60,
                        "review_count": 5 + i,
                        "prepayment_required": True,
                        "extracted_at": datetime.now().isoformat()
                    }
                    
                    booking_data.append(booking_slot)
                    
                    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
                    if len(booking_data) >= 5:
                        break
            
            # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–ª–∏, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫
            if not booking_data:
                logger.warning(f"‚ö†Ô∏è –î–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –¥–ª—è {url} - –≤–æ–∑–º–æ–∂–Ω–æ —Ç—Ä–µ–±—É–µ—Ç—Å—è JavaScript")
                booking_data = []
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö: {e}")
            booking_data = []
        
        return booking_data
    
    
    def parse_all_urls(self, urls: List[str]) -> List[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö URL"""
        all_results = []
        
        for url in urls:
            if url.strip():
                logger.info(f"üéØ –ü–∞—Ä—Å–∏–Ω–≥ URL: {url}")
                url_results = self.parse_url(url.strip())
                all_results.extend(url_results)
                
                # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                import time
                time.sleep(2)
        
        return all_results

def write_error_to_file(error_details):
    """Write detailed error information to file for debugging"""
    try:
        error_file_path = "/app/logs/supabase_errors.json"
        
        # Ensure directory exists
        os.makedirs(os.path.dirname(error_file_path), exist_ok=True)
        
        # Read existing errors
        existing_errors = []
        if os.path.exists(error_file_path):
            try:
                with open(error_file_path, 'r') as f:
                    existing_errors = json.load(f)
            except (json.JSONDecodeError, IOError):
                # File is corrupted or unreadable, start fresh
                existing_errors = []
        
        # Add new error
        existing_errors.append(error_details)
        
        # Keep only last 50 errors to prevent file from growing too large
        existing_errors = existing_errors[-50:]
        
        # Write back to file
        with open(error_file_path, 'w') as f:
            json.dump(existing_errors, f, indent=2, ensure_ascii=False)
            
        logger.info(f"üìÅ Error logged to file: {error_file_path}")
            
    except Exception as e:
        logger.error(f"‚ùå Could not write error to file: {e}")

async def save_to_database(data: List[Dict]) -> bool:
    """–ò–°–ü–†–ê–í–õ–ï–ù–û: –†–µ–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ Supabase"""
    global db_manager, parse_results
    
    try:
        logger.info(f"üíæ –†–ï–ê–õ–¨–ù–û–ï —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ {len(data)} –∑–∞–ø–∏—Å–µ–π –≤ Supabase...")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º DatabaseManager –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if db_manager is None:
            if not SUPABASE_INTEGRATION_AVAILABLE:
                logger.error("‚ùå DatabaseManager –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
                return False
                
            db_manager = DatabaseManager()
            await db_manager.initialize()
            logger.info("‚úÖ DatabaseManager –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ DatabaseManager –≥–æ—Ç–æ–≤
        if not db_manager.is_initialized:
            logger.error("‚ùå DatabaseManager –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            return False
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤ Supabase –¥–ª—è –∫–∞–∂–¥–æ–≥–æ URL
        success_count = 0
        urls_processed = set()
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ URL
        data_by_url = {}
        for item in data:
            url = item.get('url', 'unknown')
            if url not in data_by_url:
                data_by_url[url] = []
            data_by_url[url].append(item)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ URL –æ—Ç–¥–µ–ª—å–Ω–æ
        for url, url_data in data_by_url.items():
            try:
                logger.info(f"üéØ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ {len(url_data)} –∑–∞–ø–∏—Å–µ–π –¥–ª—è URL: {url}")
                success = await db_manager.save_booking_data(url, url_data)
                if success:
                    success_count += len(url_data)
                    urls_processed.add(url)
                    logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(url_data)} –∑–∞–ø–∏—Å–µ–π –¥–ª—è {url}")
                else:
                    # ENHANCED ERROR STORAGE - Store detailed save failure info
                    error_details = {
                        "url": url,
                        "error_type": "SaveFailure",
                        "error_message": "Database save returned False",
                        "timestamp": datetime.now().isoformat(),
                        "data_count": len(url_data),
                        "save_method": "db_manager.save_booking_data"
                    }
                    
                    # Store errors in parse_results for API access
                    if "database_errors" not in parse_results:
                        parse_results["database_errors"] = []
                    
                    parse_results["database_errors"].append(error_details)
                    parse_results["last_database_error"] = error_details
                    parse_results["last_error_time"] = datetime.now().isoformat()
                    parse_results["error_count"] = parse_results.get("error_count", 0) + 1
                    
                    logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è {url}")
                    
            except Exception as url_error:
                # ENHANCED ERROR STORAGE - Store detailed exception info
                error_details = {
                    "url": url,
                    "error_type": type(url_error).__name__,
                    "error_message": str(url_error),
                    "timestamp": datetime.now().isoformat(),
                    "data_count": len(url_data),
                    "exception_details": {
                        "args": getattr(url_error, 'args', []),
                        "code": getattr(url_error, 'code', None)
                    }
                }
                
                # Store errors in parse_results for API access
                if "database_errors" not in parse_results:
                    parse_results["database_errors"] = []
                
                parse_results["database_errors"].append(error_details)
                parse_results["last_database_error"] = error_details
                parse_results["last_error_time"] = datetime.now().isoformat()
                parse_results["error_count"] = parse_results.get("error_count", 0) + 1
                
                # Write error to file for persistent logging
                write_error_to_file(error_details)
                
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è URL {url}: {url_error}")
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        parse_results["total_extracted"] += success_count
        parse_results["last_data"] = data  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è API
        parse_results["last_save_time"] = datetime.now().isoformat()
        parse_results["urls_saved"] = list(urls_processed)
        parse_results["supabase_active"] = True
        
        if success_count > 0:
            logger.info(f"üéâ –£–°–ü–ï–•! –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {success_count} –∑–∞–ø–∏—Å–µ–π –≤ Supabase –¥–ª—è {len(urls_processed)} URL")
            return True
        else:
            logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–π –∑–∞–ø–∏—Å–∏")
            return False
        
    except Exception as e:
        logger.error(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –æ—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ Supabase: {e}")
        import traceback
        traceback.print_exc()
        return False

async def run_parser():
    """–ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞ YClients —Å –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–µ–π"""
    global parsing_active, last_parse_time, parse_results, db_manager
    
    if parsing_active:
        return {"status": "—É–∂–µ_–∑–∞–ø—É—â–µ–Ω"}
    
    parsing_active = True
    last_parse_time = datetime.now()
    
    try:
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞ —Å –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–µ–π...")
        
        urls = [url.strip() for url in PARSE_URLS.split(",") if url.strip()]
        if not urls:
            return {"status": "error", "message": "URL –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã"}
        
        # Initialize router with database manager
        if db_manager is None:
            if not SUPABASE_INTEGRATION_AVAILABLE:
                logger.error("‚ùå DatabaseManager –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
                return {"status": "error", "message": "DatabaseManager –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"}
            db_manager = DatabaseManager()
            await db_manager.initialize()
        
        router = ParserRouter(db_manager)
        
        all_results = []
        for url in urls:
            logger.info(f"üéØ –û–±—Ä–∞–±–æ—Ç–∫–∞ URL: {url}")
            url_results = await router.parse_url(url)
            all_results.extend(url_results)
        
        # Clean up router resources
        await router.close()
        
        if all_results:
            success = await save_to_database(all_results)
            parse_results.update({
                "status": "–∑–∞–≤–µ—Ä—à–µ–Ω–æ",
                "last_run": last_parse_time.isoformat(),
                "urls_parsed": len(urls),
                "records_extracted": len(all_results),
                "has_real_data": True,
                "no_demo_data": True
            })
            return {"status": "success", "extracted": len(all_results)}
        else:
            return {"status": "warning", "message": "–î–∞–Ω–Ω—ã–µ –Ω–µ –∏–∑–≤–ª–µ—á–µ–Ω—ã"}
            
    except Exception as e:
        parse_results["status"] = "–æ—à–∏–±–∫–∞"
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–µ—Ä–∞: {e}")
        return {"status": "error", "message": str(e)}
    
    finally:
        parsing_active = False

# API Endpoints
@app.get("/")
def read_root():
    """–ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ —Å —Å–æ—Å—Ç–æ—è–Ω–∏–µ–º –ø–∞—Ä—Å–µ—Ä–∞"""
    urls_count = len([url for url in PARSE_URLS.split(",") if url.strip()]) if PARSE_URLS else 0
    
    return HTMLResponse(f"""
    <h1>üéâ –ü–∞—Ä—Å–µ—Ä YClients - –õ—ë–≥–∫–∞—è –≤–µ—Ä—Å–∏—è!</h1>
    <p><strong>–ë–µ–∑ –±—Ä–∞—É–∑–µ—Ä–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π:</strong> –ë—ã—Å—Ç—Ä–æ –∏ –Ω–∞–¥—ë–∂–Ω–æ</p>
    
    <h3>üìä –°–æ—Å—Ç–æ—è–Ω–∏–µ –ø–∞—Ä—Å–µ—Ä–∞</h3>
    <ul>
        <li>–°—Ç–∞—Ç—É—Å: {parse_results.get('status', '–≥–æ—Ç–æ–≤')}</li>
        <li>–í—Å–µ–≥–æ URL: {urls_count}</li>
        <li>–ò–∑–≤–ª–µ—á–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {parse_results.get('total_extracted', 0)}</li>
        <li>–ü–æ—Å–ª–µ–¥–Ω–∏–π –∑–∞–ø—É—Å–∫: {parse_results.get('last_run', '–ù–∏–∫–æ–≥–¥–∞')}</li>
        <li>–í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è —Å–µ–π—á–∞—Å: {'–î–∞' if parsing_active else '–ù–µ—Ç'}</li>
    </ul>
    
    <h3>üóÑÔ∏è –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö (SUPABASE INTEGRATION)</h3>
    <ul>
        <li>–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ: {'‚úÖ –ê–∫—Ç–∏–≤–Ω–æ' if parse_results.get('supabase_active') else '‚ö†Ô∏è –ù–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–æ'}</li>
        <li>DatabaseManager: {'‚úÖ –î–æ—Å—Ç—É–ø–µ–Ω' if SUPABASE_INTEGRATION_AVAILABLE else '‚ùå –ù–µ–¥–æ—Å—Ç—É–ø–µ–Ω'}</li>
        <li>–¢–∞–±–ª–∏—Ü—ã: ‚úÖ –°–æ–∑–¥–∞–Ω—ã –≤—Ä—É—á–Ω—É—é Pavel</li>
        <li>–ü–æ—Å–ª–µ–¥–Ω–µ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ: {parse_results.get('last_save_time', '–ù–µ—Ç')}</li>
        <li>URL —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {len(parse_results.get('urls_saved', []))}</li>
    </ul>
    
    <h3>‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏</h3>
    <ul>
        <li>–ò–Ω—Ç–µ—Ä–≤–∞–ª –ø–∞—Ä—Å–∏–Ω–≥–∞: {PARSE_INTERVAL} —Å–µ–∫—É–Ω–¥</li>
        <li>–ù–∞—Å—Ç—Ä–æ–µ–Ω–æ URL: {urls_count}</li>
        <li>API –∫–ª—é—á: {'‚úÖ –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω' if API_KEY else '‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç'}</li>
        <li>–ú–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞: üöÄ Requests + BeautifulSoup (–±—ã—Å—Ç—Ä–æ –∏ –Ω–∞–¥—ë–∂–Ω–æ)</li>
    </ul>
    
    <h3>üîó API Endpoints</h3>
    <ul>
        <li><a href="/health">/health</a> - –ó–¥–æ—Ä–æ–≤—å–µ —Å–∏—Å—Ç–µ–º—ã</li>
        <li><a href="/parser/status">/parser/status</a> - –°—Ç–∞—Ç—É—Å –ø–∞—Ä—Å–µ—Ä–∞</li>
        <li><a href="/parser/run">/parser/run</a> - –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞</li>
        <li><a href="/api/booking-data">/api/booking-data</a> - –î–∞–Ω–Ω—ã–µ –±—Ä–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–π</li>
        <li><a href="/docs">/docs</a> - –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è API</li>
    </ul>
    
    <p><strong>üéØ –°—Ç–∞—Ç—É—Å:</strong> –ì–æ—Ç–æ–≤ –∫ –ø—Ä–æ–¥–∞–∫—à–Ω –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –±–µ–∑ –±—Ä–∞—É–∑–µ—Ä–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π!</p>
    """)

@app.get("/health")
def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–∏—Å—Ç–µ–º—ã"""
    return {
        "status": "ok",
        "version": "4.1.0",
        "message": "–õ—ë–≥–∫–∏–π –ø–∞—Ä—Å–µ—Ä YClients –ø–æ–ª–Ω–æ—Å—Ç—å—é —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–µ–Ω",
        "parsing_method": "requests + BeautifulSoup",
        "timestamp": datetime.now().isoformat(),
        "parser": {
            "active": parsing_active,
            "last_run": last_parse_time.isoformat() if last_parse_time else None,
            "total_extracted": parse_results.get("total_extracted", 0),
            "urls_configured": len([url for url in PARSE_URLS.split(",") if url.strip()]) if PARSE_URLS else 0
        },
        "database": {
            "connected": parse_results.get("supabase_active", False),
            "type": "SUPABASE",
            "manager_available": SUPABASE_INTEGRATION_AVAILABLE,
            "last_save": parse_results.get("last_save_time"),
            "urls_saved": parse_results.get("urls_saved", [])
        },
        "production_ready": True,
        "browser_dependencies": False
    }

@app.get("/parser/status")
def get_parser_status():
    """–ü–æ–¥—Ä–æ–±–Ω—ã–π —Å—Ç–∞—Ç—É—Å –ø–∞—Ä—Å–µ—Ä–∞"""
    urls = [url.strip() for url in PARSE_URLS.split(",") if url.strip()] if PARSE_URLS else []
    
    return {
        "parser_version": "4.1.0",
        "parsing_method": "requests + BeautifulSoup",
        "status": parse_results.get("status", "–≥–æ—Ç–æ–≤"),
        "active": parsing_active,
        "configuration": {
            "urls": urls,
            "url_count": len(urls),
            "parse_interval": PARSE_INTERVAL,
            "auto_parsing": True
        },
        "statistics": {
            "total_extracted": parse_results.get("total_extracted", 0),
            "last_run": parse_results.get("last_run"),
            "last_extraction_count": parse_results.get("records_extracted", 0)
        },
        "next_run": "–†—É—á–Ω–æ–π –∑–∞–ø—É—Å–∫ –∏–ª–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π",
        "ready": bool(urls and SUPABASE_URL and SUPABASE_KEY),
        "browser_dependencies": False
    }

@app.post("/parser/run")
async def run_parser_manually():
    """–†—É—á–Ω–æ–π –∑–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞"""
    result = await run_parser()
    return result

@app.get("/api/booking-data")
def get_booking_data(
    limit: int = Query(50, description="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π"),
    offset: int = Query(0, description="–°–º–µ—â–µ–Ω–∏–µ –¥–ª—è –ø–∞–≥–∏–Ω–∞—Ü–∏–∏")
):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –±—Ä–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–π"""
    
    last_data = parse_results.get("last_data", [])
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–∞–≥–∏–Ω–∞—Ü–∏—é
    paginated_data = last_data[offset:offset + limit]
    
    return {
        "status": "success",
        "total": len(last_data),
        "limit": limit,
        "offset": offset,
        "data": paginated_data,
        "parser_info": {
            "parsing_method": "requests + BeautifulSoup",
            "last_updated": parse_results.get("last_save_time"),
            "total_records": parse_results.get("total_extracted", 0),
            "urls_parsed": len([url for url in PARSE_URLS.split(",") if url.strip()]) if PARSE_URLS else 0
        }
    }

@app.get("/api/urls")
def get_configured_urls():
    """–°–ø–∏—Å–æ–∫ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã—Ö URL"""
    urls = [url.strip() for url in PARSE_URLS.split(",") if url.strip()] if PARSE_URLS else []
    
    return {
        "urls": urls,
        "count": len(urls),
        "status": "–Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã" if urls else "–Ω–µ_–Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã"
    }

# –î–ò–ê–ì–ù–û–°–¢–ò–ß–ï–°–ö–ò–ï –≠–ù–î–ü–û–ò–ù–¢–´ - Exposing detailed error information programmatically
@app.get("/diagnostics/errors")
def get_error_diagnostics():
    """Get detailed error information for debugging"""
    return {
        "last_errors": parse_results.get("last_errors", []),
        "error_count": parse_results.get("error_count", 0),
        "last_error_time": parse_results.get("last_error_time"),
        "database_errors": parse_results.get("database_errors", []),
        "supabase_connection_status": parse_results.get("supabase_active", False),
        "last_save_attempt": parse_results.get("last_save_time"),
        "detailed_diagnostics": parse_results.get("detailed_diagnostics", {}),
        "last_database_error": parse_results.get("last_database_error"),
        "last_error_details": parse_results.get("last_error_details"),
        "diagnostic_available": True,
        "timestamp": datetime.now().isoformat()
    }

@app.get("/diagnostics/test-save")
async def test_database_save():
    """Test database save operation and return detailed results"""
    global db_manager, parse_results
    
    if db_manager is None:
        return {
            "error": "DatabaseManager not initialized",
            "available": False,
            "timestamp": datetime.now().isoformat()
        }
    
    test_data = [{
        "url": "diagnostic_test",
        "date": "2025-07-15",
        "time": "10:00",
        "price": "test_price",
        "provider": "diagnostic_test_provider",
        "seat_number": "1",
        "location_name": "test_location",
        "court_type": "TEST",
        "time_category": "–î–ï–ù–¨",
        "duration": 60,
        "review_count": 0,
        "prepayment_required": False,
        "extracted_at": datetime.now().isoformat()
    }]
    
    try:
        logger.info("üß™ DIAGNOSTIC: Testing database save operation...")
        success = await db_manager.save_booking_data("diagnostic_test", test_data)
        
        result = {
            "test_save_success": success,
            "last_error": parse_results.get("last_database_error"),
            "error_details": parse_results.get("last_error_details"),
            "supabase_active": parse_results.get("supabase_active", False),
            "database_manager_initialized": db_manager.is_initialized if db_manager else False,
            "test_data_sent": test_data,
            "timestamp": datetime.now().isoformat()
        }
        
        if success:
            logger.info("‚úÖ DIAGNOSTIC: Test save successful")
        else:
            logger.error("‚ùå DIAGNOSTIC: Test save failed")
            
        return result
        
    except Exception as e:
        error_info = {
            "test_save_success": False,
            "exception": str(e),
            "exception_type": type(e).__name__,
            "timestamp": datetime.now().isoformat(),
            "database_manager_available": db_manager is not None,
            "database_manager_initialized": db_manager.is_initialized if db_manager else False
        }
        
        # Store diagnostic error
        parse_results["last_diagnostic_error"] = error_info
        logger.error(f"‚ùå DIAGNOSTIC: Exception during test save: {e}")
        
        return error_info

@app.get("/diagnostics/error-log")
def get_error_log():
    """Read error log file"""
    try:
        error_file_path = "/app/logs/supabase_errors.json"
        if os.path.exists(error_file_path):
            with open(error_file_path, 'r') as f:
                errors = json.load(f)
            return {
                "errors": errors, 
                "count": len(errors),
                "file_path": error_file_path,
                "file_exists": True
            }
        else:
            return {
                "errors": [], 
                "count": 0, 
                "message": "No error log file found",
                "file_path": error_file_path,
                "file_exists": False
            }
    except Exception as e:
        return {
            "error": f"Could not read error log: {e}",
            "file_path": "/app/logs/supabase_errors.json",
            "exception_type": type(e).__name__
        }

@app.get("/diagnostics/system")
def get_system_diagnostics():
    """Get comprehensive system diagnostic information"""
    return {
        "environment": {
            "supabase_url_set": bool(SUPABASE_URL),
            "supabase_key_set": bool(SUPABASE_KEY),
            "parse_urls_set": bool(PARSE_URLS),
            "api_key_set": bool(API_KEY)
        },
        "database": {
            "manager_available": SUPABASE_INTEGRATION_AVAILABLE,
            "manager_initialized": db_manager.is_initialized if db_manager else False,
            "connection_active": parse_results.get("supabase_active", False),
            "last_save_attempt": parse_results.get("last_save_time"),
            "urls_saved_count": len(parse_results.get("urls_saved", []))
        },
        "parser": {
            "active": parsing_active,
            "last_run": last_parse_time.isoformat() if last_parse_time else None,
            "total_extracted": parse_results.get("total_extracted", 0),
            "urls_configured": len([url for url in PARSE_URLS.split(",") if url.strip()]) if PARSE_URLS else 0
        },
        "errors": {
            "error_count": parse_results.get("error_count", 0),
            "last_error_time": parse_results.get("last_error_time"),
            "database_errors_count": len(parse_results.get("database_errors", [])),
            "has_diagnostic_errors": "last_diagnostic_error" in parse_results
        },
        "timestamp": datetime.now().isoformat()
    }

async def background_parser_task():
    """–§–æ–Ω–æ–≤–∞—è –∑–∞–¥–∞—á–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞–∂–¥—ã–µ 10 –º–∏–Ω—É—Ç"""
    logger.info(f"üîÑ –ó–∞–ø—É—Å–∫ —Ñ–æ–Ω–æ–≤–æ–π –∑–∞–¥–∞—á–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ (–∏–Ω—Ç–µ—Ä–≤–∞–ª: {PARSE_INTERVAL} —Å–µ–∫—É–Ω–¥)")
    
    while True:
        try:
            if not parsing_active:  # –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ–º –ø–µ—Ä–µ–∫—Ä—ã–≤–∞—é—â–∏–µ—Å—è –∑–∞–ø—É—Å–∫–∏
                logger.info("üîÑ –ù–∞—á–∞–ª–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞...")
                await run_parser()
                logger.info(f"‚è∞ –°–ª–µ–¥—É—é—â–∏–π –ø–∞—Ä—Å–∏–Ω–≥ —á–µ—Ä–µ–∑ {PARSE_INTERVAL} —Å–µ–∫—É–Ω–¥")
            else:
                logger.info("‚è≥ –ü–∞—Ä—Å–∏–Ω–≥ —É–∂–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º...")
            
            await asyncio.sleep(PARSE_INTERVAL)
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ñ–æ–Ω–æ–≤–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞: {e}")
            await asyncio.sleep(60)  # –ñ–¥—ë–º 1 –º–∏–Ω—É—Ç—É –ø—Ä–∏ –æ—à–∏–±–∫–µ

async def run_api_server():
    """–ó–∞–ø—É—Å–∫ API —Å–µ—Ä–≤–µ—Ä–∞ –∫–∞–∫ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –∑–∞–¥–∞—á–∏"""
    config = uvicorn.Config(
        app=app,
        host=API_HOST,
        port=API_PORT,
        log_level="info"
    )
    server = uvicorn.Server(config)
    await server.serve()

if __name__ == "__main__":
    print(f"üöÄ –£–õ–£–ß–®–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø: –ü–∞—Ä—Å–µ—Ä YClients –ë–ï–ó –î–ï–ú–û-–î–ê–ù–ù–´–•")
    print(f"üìã –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏—Å—Ç–µ–º—ã:")
    print(f"   - API_KEY: {'‚úÖ –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω' if API_KEY else '‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç'}")
    print(f"   - PARSE_URLS: {'‚úÖ –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω' if PARSE_URLS else '‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç'}")
    print(f"   - SUPABASE_URL: {'‚úÖ –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω' if SUPABASE_URL else '‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç'}")
    print(f"   - SUPABASE_KEY: {'‚úÖ –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω' if SUPABASE_KEY else '‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç'}")
    
    urls = [url.strip() for url in PARSE_URLS.split(",") if url.strip()] if PARSE_URLS else []
    print(f"üéØ URL –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞: {len(urls)}")
    for i, url in enumerate(urls, 1):
        print(f"   {i}. {url}")
    
    print(f"üèÅ –ì–û–¢–û–í–ù–û–°–¢–¨ –ö –ü–†–û–î–ê–ö–®–ù: {'‚úÖ –î–ê' if all([API_KEY, PARSE_URLS, SUPABASE_URL, SUPABASE_KEY]) else '‚ùå –ù–ï–¢'}")
    print(f"üöÄ –ò–∑–º–µ–Ω–µ–Ω–∏—è: –ù–ï–¢ –¥–µ–º–æ-–¥–∞–Ω–Ω—ã—Ö + –∞–≤—Ç–æ–ø–∞—Ä—Å–∏–Ω–≥ + JavaScript –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ")
    print(f"üí° JavaScript-—Ç—è–∂–µ–ª—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Ç—Ä–µ–±—É—é—Ç Playwright-–ø–∞—Ä—Å–µ—Ä")
    
    try:
        # –ó–∞–ø—É—Å–∫–∞–µ–º API —Å–µ—Ä–≤–µ—Ä –∏ —Ñ–æ–Ω–æ–≤—ã–π –ø–∞—Ä—Å–µ—Ä –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
        asyncio.run(asyncio.gather(
            run_api_server(),
            background_parser_task()
        ))
    except KeyboardInterrupt:
        print("\nüëã –ü–∞—Ä—Å–µ—Ä –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        print(f"\nüí• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
